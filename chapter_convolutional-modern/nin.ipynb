{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a20c0bc",
      "metadata": {
        "id": "8a20c0bc"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff191e18",
      "metadata": {
        "id": "ff191e18"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/d2l-ai/d2l-zh@release  # installing d2l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "324028bf",
      "metadata": {
        "origin_pos": 0,
        "id": "324028bf"
      },
      "source": [
        "# 网络中的网络（NiN）\n",
        ":label:`sec_nin`\n",
        "\n",
        "LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。\n",
        "AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。\n",
        "或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。\n",
        "*网络中的网络*（*NiN*）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机 :cite:`Lin.Chen.Yan.2013`\n",
        "\n",
        "## (**NiN块**)\n",
        "\n",
        "回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。\n",
        "另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。\n",
        "NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。\n",
        "如果我们将权重连接到每个空间位置，我们可以将其视为$1\\times 1$卷积层（如 :numref:`sec_channels`中所述），或作为在每个像素位置上独立作用的全连接层。\n",
        "从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。\n",
        "\n",
        " :numref:`fig_nin`说明了VGG和NiN及它们的块之间主要架构差异。\n",
        "NiN块以一个普通卷积层开始，后面是两个$1 \\times 1$的卷积层。这两个$1 \\times 1$卷积层充当带有ReLU激活函数的逐像素全连接层。\n",
        "第一层的卷积窗口形状通常由用户设置。\n",
        "随后的卷积窗口形状固定为$1 \\times 1$。\n",
        "\n",
        "![对比 VGG 和 NiN 及它们的块之间主要架构差异。](http://d2l.ai/_images/nin.svg)\n",
        ":width:`600px`\n",
        ":label:`fig_nin`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9b116832",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:19:49.388461Z",
          "iopub.status.busy": "2023-08-18T07:19:49.387694Z",
          "iopub.status.idle": "2023-08-18T07:19:52.241405Z",
          "shell.execute_reply": "2023-08-18T07:19:52.240502Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "9b116832"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# from d2l import torch as d2l\n",
        "\n",
        "\n",
        "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc11a427",
      "metadata": {
        "origin_pos": 5,
        "id": "fc11a427"
      },
      "source": [
        "## [**NiN模型**]\n",
        "\n",
        "最初的NiN网络是在AlexNet后不久提出的，显然从中得到了一些启示。\n",
        "NiN使用窗口形状为$11\\times 11$、$5\\times 5$和$3\\times 3$的卷积层，输出通道数量与AlexNet中的相同。\n",
        "每个NiN块后有一个最大汇聚层，汇聚窗口形状为$3\\times 3$，步幅为2。\n",
        "\n",
        "NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。\n",
        "相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个*全局平均汇聚层*（global average pooling layer），生成一个对数几率\t（logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8ba4ca30",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:19:52.246912Z",
          "iopub.status.busy": "2023-08-18T07:19:52.246237Z",
          "iopub.status.idle": "2023-08-18T07:19:52.294688Z",
          "shell.execute_reply": "2023-08-18T07:19:52.293840Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "8ba4ca30"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n",
        "    nn.MaxPool2d(3, stride=2),\n",
        "    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
        "    nn.MaxPool2d(3, stride=2),\n",
        "    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
        "    nn.MaxPool2d(3, stride=2),\n",
        "    nn.Dropout(0.5),\n",
        "    # 标签类别数是10\n",
        "    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "    # 将四维的输出转成二维的输出，其形状为(批量大小,10)\n",
        "    nn.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c9e9ac",
      "metadata": {
        "origin_pos": 10,
        "id": "76c9e9ac"
      },
      "source": [
        "我们创建一个数据样本来[**查看每个块的输出形状**]。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2917eb46",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:19:52.298946Z",
          "iopub.status.busy": "2023-08-18T07:19:52.298371Z",
          "iopub.status.idle": "2023-08-18T07:19:52.325792Z",
          "shell.execute_reply": "2023-08-18T07:19:52.324591Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "2917eb46",
        "outputId": "dd4f17fd-c01a-4b1c-9961-ab9cec76c4c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
            "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
            "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
            "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
            "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
            "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
            "Flatten output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(size=(1, 1, 224, 224))\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95a4c245",
      "metadata": {
        "origin_pos": 15,
        "id": "95a4c245"
      },
      "source": [
        "## [**训练模型**]\n",
        "\n",
        "和以前一样，我们使用Fashion-MNIST来训练模型。训练NiN与训练AlexNet、VGG时相似。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3c362e21",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:19:52.329626Z",
          "iopub.status.busy": "2023-08-18T07:19:52.329083Z",
          "iopub.status.idle": "2023-08-18T07:24:13.568006Z",
          "shell.execute_reply": "2023-08-18T07:24:13.566948Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "3c362e21",
        "outputId": "5f4ef09c-a51e-4aec-ed6f-020b1ee08d08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "lr, num_epochs, batch_size = 0.1, 10, 128\n",
        "batch_size = 128\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "def load_data_fashion_mnist(batch_size, resize=None):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    trans = [transforms.ToTensor()]\n",
        "    if resize:\n",
        "        trans.insert(0, transforms.Resize(resize))\n",
        "    trans = transforms.Compose(trans)\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=True, transform=trans, download=True)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=False, transform=trans, download=True)\n",
        "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                                        num_workers=4),\n",
        "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
        "                                        num_workers=4))\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
        "    \"\"\"Train a model with a GPU (defined in Chapter 6).\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    print('training on', device)\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Sum of training loss, sum of training accuracy, no. of examples\n",
        "        train_loss_sum = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        net.train()\n",
        "        for X, y in train_iter:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_sum += l.item() * y.size(0)\n",
        "            _, predicted = torch.max(y_hat, 1)\n",
        "            train_correct += (predicted == y).sum().item()\n",
        "            train_total += y.size(0)\n",
        "            train_loss = train_loss_sum / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # ---- evaluate test accuracy ----\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "              f\"- Loss: {train_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Test Acc: {test_acc:.4f}\")\n",
        "def evaluate_accuracy_gpu(net, data_iter, device):\n",
        "    net.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            _, predicted = torch.max(y_hat, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "def gpu(i=0):\n",
        "    \"\"\"Get a GPU device.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.device(f'cuda:{i}')\n",
        "def cpu():\n",
        "    \"\"\"Get the CPU device.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.device('cpu')\n",
        "def num_gpus():\n",
        "    \"\"\"Get the number of available GPUs.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.cuda.device_count()\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    if num_gpus() >= i + 1:\n",
        "        return gpu(i)\n",
        "    return cpu()\n",
        "\n",
        "train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
      ],
      "metadata": {
        "id": "Bi38jjHnQwxF",
        "outputId": "0a72ea3c-e9d4-4943-cb93-aaeff0af9730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "id": "Bi38jjHnQwxF",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda:0\n",
            "Epoch 1/10 - Loss: 2.1290, Train Acc: 0.1952, Test Acc: 0.2865\n",
            "Epoch 2/10 - Loss: 1.8403, Train Acc: 0.3106, Test Acc: 0.3303\n",
            "Epoch 3/10 - Loss: 1.7673, Train Acc: 0.3453, Test Acc: 0.3324\n",
            "Epoch 4/10 - Loss: 2.0502, Train Acc: 0.2192, Test Acc: 0.1000\n",
            "Epoch 5/10 - Loss: 2.3026, Train Acc: 0.1000, Test Acc: 0.1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4054007283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrain_ch6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4054007283.py\u001b[0m in \u001b[0;36mtrain_ch6\u001b[0;34m(net, train_iter, test_iter, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtrain_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtrain_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "847261ce",
      "metadata": {
        "origin_pos": 17,
        "id": "847261ce"
      },
      "source": [
        "## 小结\n",
        "\n",
        "* NiN使用由一个卷积层和多个$1\\times 1$卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。\n",
        "* NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。\n",
        "* 移除全连接层可减少过拟合，同时显著减少NiN的参数。\n",
        "* NiN的设计影响了许多后续卷积神经网络的设计。\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 调整NiN的超参数，以提高分类准确性。\n",
        "1. 为什么NiN块中有两个$1\\times 1$卷积层？删除其中一个，然后观察和分析实验现象。\n",
        "1. 计算NiN的资源使用情况。\n",
        "    1. 参数的数量是多少？\n",
        "    1. 计算量是多少？\n",
        "    1. 训练期间需要多少显存？\n",
        "    1. 预测期间需要多少显存？\n",
        "1. 一次性直接将$384 \\times 5 \\times 5$的表示缩减为$10 \\times 5 \\times 5$的表示，会存在哪些问题？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288616e8",
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "288616e8"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1869)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}