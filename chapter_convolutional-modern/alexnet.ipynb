{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18b6067d",
      "metadata": {
        "id": "18b6067d"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c22252e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c22252e0",
        "outputId": "c153359b-27e8-4107-c5bb-2ba81b331044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Using cached d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Collecting jupyter==1.0.0 (from d2l)\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting numpy==1.23.5 (from d2l)\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m No available output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Failed to build 'numpy' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install d2l"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0f1fb9d",
        "outputId": "74e4a5d3-d94a-4fd2-9b1e-ae97d1d0f1f5"
      },
      "source": [],
      "id": "f0f1fb9d",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076ca4f2",
      "metadata": {
        "origin_pos": 0,
        "id": "076ca4f2"
      },
      "source": [
        "# 深度卷积神经网络（AlexNet）\n",
        ":label:`sec_alexnet`\n",
        "\n",
        "在LeNet提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但卷积神经网络并没有主导这些领域。这是因为虽然LeNet在小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究。事实上，在上世纪90年代初到2012年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机（support vector machines）。\n",
        "\n",
        "在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。\n",
        "\n",
        "虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。\n",
        "\n",
        "因此，与训练*端到端*（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：\n",
        "\n",
        "1. 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）。\n",
        "2. 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。\n",
        "3. 通过标准的特征提取算法，如SIFT（尺度不变特征变换） :cite:`Lowe.2004`和SURF（加速鲁棒特征） :cite:`Bay.Tuytelaars.Van-Gool.2006`或其他手动调整的流水线来输入数据。\n",
        "4. 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。\n",
        "\n",
        "当人们和机器学习研究人员交谈时，会发现机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而，当人们和计算机视觉研究人员交谈，会听到一个完全不同的故事。计算机视觉研究人员会告诉一个诡异事实————推动领域进步的是数据特征，而不是学习算法。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。\n",
        "\n",
        "## 学习表征\n",
        "\n",
        "另一种预测这个领域发展的方法————观察图像特征的提取方法。在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。SIFT :cite:`Lowe.2004`、SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`、HOG（定向梯度直方图） :cite:`Dalal.Triggs.2005`、[bags of visual words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)和类似的特征提取方法占据了主导地位。\n",
        "\n",
        "另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，想法则与众不同：他们认为特征本身应该被学习。此外，他们还认为，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。事实上，Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体*AlexNet*。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，他是论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`的第一作者。\n",
        "\n",
        "有趣的是，在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。 :numref:`fig_filters`是从AlexNet论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`复制的，描述了底层图像特征。\n",
        "\n",
        "![AlexNet第一层学习到的特征抽取器。](https://github.com/d2l-ai/d2l-zh-pytorch-colab/blob/master/img/filters.png?raw=1)\n",
        ":width:`400px`\n",
        ":label:`fig_filters`\n",
        "\n",
        "AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如眼睛、鼻子、草叶等等。而更高的层可以检测整个物体，如人、飞机、狗或飞盘。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。\n",
        "\n",
        "### 缺少的成分：数据\n",
        "\n",
        "包含许多特征的深度模型需要大量的有标签数据，才能显著优于基于凸优化的传统方法（如线性方法和核方法）。\n",
        "然而，限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张在非自然环境下以低分辨率拍摄的图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。\n",
        "\n",
        "### 缺少的成分：硬件\n",
        "\n",
        "深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据。这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。然而，用GPU训练神经网络改变了这一格局。*图形处理器*（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的$4 \\times 4$矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为*通用GPU*（general-purpose GPUs，GPGPU）来销售。\n",
        "\n",
        "那么GPU比CPU强在哪里呢？\n",
        "\n",
        "首先，我们深度理解一下中央处理器（Central Processing Unit，CPU）的*核心*。\n",
        "CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。\n",
        "它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。\n",
        "然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。\n",
        "它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。\n",
        "现代笔记本电脑最多有4核，即使是高端服务器也很少超过64核，因为它们的性价比不高。\n",
        "\n",
        "相比于CPU，GPU由$100 \\sim 1000$个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。\n",
        "虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级。\n",
        "例如，NVIDIA最近一代的Ampere GPU架构为每个芯片提供了高达312 TFlops的浮点性能，而CPU的浮点性能到目前为止还没有超过1 TFlops。\n",
        "之所以有如此大的差距，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。\n",
        "对于一个CPU核心，假设它的运行速度比GPU快4倍，但可以使用16个GPU核代替，那么GPU的综合性能就是CPU的$16 \\times 1/4 = 4$倍。\n",
        "其次，GPU内核要简单得多，这使得它们更节能。\n",
        "此外，深度学习中的许多操作需要相对较高的内存带宽，而GPU拥有10倍于CPU的带宽。\n",
        "\n",
        "回到2012年的重大突破，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。\n",
        "于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)几年来它一直是行业标准，并推动了深度学习热潮。\n",
        "\n",
        "## AlexNet\n",
        "\n",
        "2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。\n",
        "AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。\n",
        "\n",
        "AlexNet和LeNet的架构非常相似，如 :numref:`fig_alexnet`所示。\n",
        "注意，本书在这里提供的是一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。\n",
        "\n",
        "![从LeNet（左）到AlexNet（右）](http://d2l.ai/_images/alexnet.svg)\n",
        ":label:`fig_alexnet`\n",
        "\n",
        "AlexNet和LeNet的设计理念非常相似，但也存在显著差异。\n",
        "\n",
        "1. AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。\n",
        "2. AlexNet使用ReLU而不是sigmoid作为其激活函数。\n",
        "\n",
        "下面的内容将深入研究AlexNet的细节。\n",
        "\n",
        "### 模型设计\n",
        "\n",
        "在AlexNet的第一层，卷积窗口的形状是$11\\times11$。\n",
        "由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。\n",
        "第二层中的卷积窗口形状被缩减为$5\\times5$，然后是$3\\times3$。\n",
        "此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为$3\\times3$、步幅为2的最大汇聚层。\n",
        "而且，AlexNet的卷积通道数目是LeNet的10倍。\n",
        "\n",
        "在最后一个卷积层后有两个全连接层，分别有4096个输出。\n",
        "这两个巨大的全连接层拥有将近1GB的模型参数。\n",
        "由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。\n",
        "幸运的是，现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型（因此，本书的AlexNet模型在这方面与原始论文稍有不同）。\n",
        "\n",
        "### 激活函数\n",
        "\n",
        "此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。\n",
        "一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。\n",
        "另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。\n",
        "当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。\n",
        "相反，ReLU激活函数在正区间的梯度总是1。\n",
        "因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。\n",
        "\n",
        "### 容量控制和预处理\n",
        "\n",
        "AlexNet通过暂退法（ :numref:`sec_dropout`）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。\n",
        "为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。\n",
        "这使得模型更健壮，更大的样本量有效地减少了过拟合。\n",
        "在 :numref:`sec_image_augmentation`中更详细地讨论数据扩增。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0c793505",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:25:53.331824Z",
          "iopub.status.busy": "2023-08-18T07:25:53.331288Z",
          "iopub.status.idle": "2023-08-18T07:25:56.717289Z",
          "shell.execute_reply": "2023-08-18T07:25:56.716283Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "0c793505"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
        "import torch\n",
        "from torch import nn\n",
        "# from d2l import torch as d2l\n",
        "\n",
        "net = nn.Sequential(\n",
        "    # 这里使用一个11*11的更大窗口来捕捉对象。\n",
        "    # 同时，步幅为4，以减少输出的高度和宽度。\n",
        "    # 另外，输出通道的数目远大于LeNet\n",
        "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
        "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
        "    # 除了最后的卷积层，输出通道的数量进一步增加。\n",
        "    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
        "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    nn.Flatten(),\n",
        "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
        "    nn.Linear(6400, 4096), nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(4096, 4096), nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),\n",
        "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
        "    nn.Linear(4096, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2dde541",
      "metadata": {
        "origin_pos": 5,
        "id": "c2dde541"
      },
      "source": [
        "[**我们构造一个**]高度和宽度都为224的(**单通道数据，来观察每一层输出的形状**)。\n",
        "它与 :numref:`fig_alexnet`中的AlexNet架构相匹配。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "77cc7ef4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:25:56.723055Z",
          "iopub.status.busy": "2023-08-18T07:25:56.722359Z",
          "iopub.status.idle": "2023-08-18T07:25:56.778616Z",
          "shell.execute_reply": "2023-08-18T07:25:56.777770Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77cc7ef4",
        "outputId": "e44f5e40-9dcd-4445-d8a1-afdfad76e126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
            "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
            "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
            "Flatten output shape:\t torch.Size([1, 6400])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "X = torch.randn(1, 1, 224, 224)\n",
        "for layer in net:\n",
        "    X=layer(X)\n",
        "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cf9d2fb",
      "metadata": {
        "origin_pos": 10,
        "id": "0cf9d2fb"
      },
      "source": [
        "## 读取数据集\n",
        "\n",
        "尽管原文中AlexNet是在ImageNet上进行训练的，但本书在这里使用的是Fashion-MNIST数据集。因为即使在现代GPU上，训练ImageNet模型，同时使其收敛可能需要数小时或数天的时间。\n",
        "将AlexNet直接应用于Fashion-MNIST的一个问题是，[**Fashion-MNIST图像的分辨率**]（$28 \\times 28$像素）(**低于ImageNet图像。**)\n",
        "为了解决这个问题，(**我们将它们增加到$224 \\times 224$**)（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用AlexNet架构）。\n",
        "这里需要使用`d2l.load_data_fashion_mnist`函数中的`resize`参数执行此调整。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d1afceea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:25:56.782593Z",
          "iopub.status.busy": "2023-08-18T07:25:56.782045Z",
          "iopub.status.idle": "2023-08-18T07:25:56.870711Z",
          "shell.execute_reply": "2023-08-18T07:25:56.869484Z"
        },
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1afceea",
        "outputId": "4aa35f27-f454-4523-b469-0dd9311e498a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 205kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.77MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.0MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "def load_data_fashion_mnist(batch_size, resize=None):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    trans = [transforms.ToTensor()]\n",
        "    if resize:\n",
        "        trans.insert(0, transforms.Resize(resize))\n",
        "    trans = transforms.Compose(trans)\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=True, transform=trans, download=True)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=False, transform=trans, download=True)\n",
        "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                                        num_workers=4),\n",
        "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
        "                                        num_workers=4))\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17f94523",
      "metadata": {
        "origin_pos": 12,
        "id": "17f94523"
      },
      "source": [
        "## [**训练AlexNet**]\n",
        "\n",
        "现在AlexNet可以开始被训练了。与 :numref:`sec_lenet`中的LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "1bMhJ4fBguYa"
      },
      "id": "1bMhJ4fBguYa",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1737d1ba",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:25:56.876133Z",
          "iopub.status.busy": "2023-08-18T07:25:56.875855Z",
          "iopub.status.idle": "2023-08-18T07:29:34.981913Z",
          "shell.execute_reply": "2023-08-18T07:29:34.980933Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1737d1ba",
        "outputId": "43b602b9-adb6-4445-e265-82a0e8b7194e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda:0\n",
            "Epoch 1/10 - Loss: 1.3542, Train Acc: 0.4939, Test Acc: 0.7033\n",
            "Epoch 2/10 - Loss: 0.6518, Train Acc: 0.7554, Test Acc: 0.7880\n",
            "Epoch 3/10 - Loss: 0.5406, Train Acc: 0.8001, Test Acc: 0.8171\n",
            "Epoch 4/10 - Loss: 0.4753, Train Acc: 0.8246, Test Acc: 0.8404\n",
            "Epoch 5/10 - Loss: 0.4332, Train Acc: 0.8393, Test Acc: 0.8503\n",
            "Epoch 6/10 - Loss: 0.4031, Train Acc: 0.8523, Test Acc: 0.8561\n",
            "Epoch 7/10 - Loss: 0.3787, Train Acc: 0.8611, Test Acc: 0.8626\n",
            "Epoch 8/10 - Loss: 0.3602, Train Acc: 0.8685, Test Acc: 0.8692\n",
            "Epoch 9/10 - Loss: 0.3423, Train Acc: 0.8750, Test Acc: 0.8787\n",
            "Epoch 10/10 - Loss: 0.3292, Train Acc: 0.8799, Test Acc: 0.8823\n"
          ]
        }
      ],
      "source": [
        "lr, num_epochs = 0.01, 10\n",
        "\n",
        "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
        "    \"\"\"Train a model with a GPU (defined in Chapter 6).\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    print('training on', device)\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Sum of training loss, sum of training accuracy, no. of examples\n",
        "        train_loss_sum = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        net.train()\n",
        "        for X, y in train_iter:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_sum += l.item() * y.size(0)\n",
        "            _, predicted = torch.max(y_hat, 1)\n",
        "            train_correct += (predicted == y).sum().item()\n",
        "            train_total += y.size(0)\n",
        "            train_loss = train_loss_sum / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # ---- evaluate test accuracy ----\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "              f\"- Loss: {train_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Test Acc: {test_acc:.4f}\")\n",
        "def evaluate_accuracy_gpu(net, data_iter, device):\n",
        "    net.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            _, predicted = torch.max(y_hat, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "def gpu(i=0):\n",
        "    \"\"\"Get a GPU device.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.device(f'cuda:{i}')\n",
        "def cpu():\n",
        "    \"\"\"Get the CPU device.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.device('cpu')\n",
        "def num_gpus():\n",
        "    \"\"\"Get the number of available GPUs.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.cuda.device_count()\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    if num_gpus() >= i + 1:\n",
        "        return gpu(i)\n",
        "    return cpu()\n",
        "\n",
        "train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qCYwSFjk5Ty"
      },
      "id": "8qCYwSFjk5Ty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ec1dd27a",
      "metadata": {
        "origin_pos": 14,
        "id": "ec1dd27a"
      },
      "source": [
        "## 小结\n",
        "\n",
        "* AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。\n",
        "* 今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。\n",
        "* 尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。\n",
        "* Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 试着增加迭代轮数。对比LeNet的结果有什么不同？为什么？\n",
        "1. AlexNet对Fashion-MNIST数据集来说可能太复杂了。\n",
        "    1. 尝试简化模型以加快训练速度，同时确保准确性不会显著下降。\n",
        "    1. 设计一个更好的模型，可以直接在$28 \\times 28$图像上工作。\n",
        "1. 修改批量大小，并观察模型精度和GPU显存变化。\n",
        "1. 分析了AlexNet的计算性能。\n",
        "    1. 在AlexNet中主要是哪部分占用显存？\n",
        "    1. 在AlexNet中主要是哪部分需要更多的计算？\n",
        "    1. 计算结果时显存带宽如何？\n",
        "1. 将dropout和ReLU应用于LeNet-5，效果有提升吗？再试试预处理会怎么样？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7add4d8d",
      "metadata": {
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "7add4d8d"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1863)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73b9a57a"
      },
      "source": [
        "# Task\n",
        "Fix the installation of `d2l` and `numpy` packages to resolve compatibility issues."
      ],
      "id": "73b9a57a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37474ae3"
      },
      "source": [
        "## Uninstall problematic packages\n",
        "\n",
        "### Subtask:\n",
        "Uninstall any existing `d2l` and `numpy` packages to ensure a clean installation environment.\n"
      ],
      "id": "37474ae3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09fae8d8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires uninstalling the `d2l` package to ensure a clean installation environment. This is the first step in the instructions.\n",
        "\n"
      ],
      "id": "09fae8d8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b50e01d",
        "outputId": "9d045b03-97ed-4b71-98f6-48b2b8013b5c"
      },
      "source": [
        "get_ipython().system('pip uninstall -y d2l')"
      ],
      "id": "6b50e01d",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping d2l as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54d751d4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires uninstalling the `numpy` package to ensure a clean installation environment. This is the second step in the instructions.\n",
        "\n"
      ],
      "id": "54d751d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bdf2832",
        "outputId": "9b285024-0e7c-46e5-cbf0-8bb4430eee51"
      },
      "source": [
        "get_ipython().system('pip uninstall -y numpy')"
      ],
      "id": "0bdf2832",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4c55692"
      },
      "source": [
        "## Install compatible NumPy\n",
        "\n",
        "### Subtask:\n",
        "Install a `numpy` version that is compatible with the current Python 3.12 environment, allowing `pip` to select the latest suitable version.\n"
      ],
      "id": "a4c55692"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33dc3ac2"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing the `numpy` package, allowing pip to select the latest compatible version for Python 3.12. This is achieved by running `pip install numpy`.\n",
        "\n"
      ],
      "id": "33dc3ac2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "7c3b9ffd",
        "outputId": "395132b0-a8bd-495b-ab6a-a7f55705afd5"
      },
      "source": [
        "get_ipython().system('pip install numpy')"
      ],
      "id": "7c3b9ffd",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "6d03888954884dc493a1f18cb1654d9d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b4871dd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `numpy` resulted in version `2.3.5`, which caused dependency conflicts with other installed packages like `tensorflow`, `opencv`, and `numba`. To resolve this, I will first uninstall the conflicting `numpy` version and then install a specific compatible version of `numpy`, such as `1.26.0`, which satisfies the dependency requirements of the other packages.\n",
        "\n"
      ],
      "id": "4b4871dd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "e103506c",
        "outputId": "ae1b2a01-5965-42e4-8f9a-c49a463c2988"
      },
      "source": [
        "get_ipython().system('pip uninstall -y numpy')\n",
        "get_ipython().system('pip install numpy==1.26.0')"
      ],
      "id": "e103506c",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.3.5\n",
            "Uninstalling numpy-2.3.5:\n",
            "  Successfully uninstalled numpy-2.3.5\n",
            "Collecting numpy==1.26.0\n",
            "  Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "music21 9.9.1 requires numpy>=1.26.4, but you have numpy 1.26.0 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "33b4f8443a7f47eb89a34fbaad8401b9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8b19029"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `numpy` (version `1.26.0`) introduced new dependency conflicts with other pre-existing packages in the environment, indicating that a universally compatible version is hard to find through simple `pip install numpy` or by picking `1.26.0`. To address the original `d2l` installation issue, which failed while trying to build `numpy==1.23.5`, the next step is to explicitly uninstall the current `numpy` and then attempt to install `numpy==1.23.5`. This directly targets the `numpy` version that `d2l` seemingly requires.\n",
        "\n"
      ],
      "id": "b8b19029"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90b2b334",
        "outputId": "f1e1cd1d-c49a-43b1-cfdc-615402ba9773"
      },
      "source": [
        "get_ipython().system('pip uninstall -y numpy')\n",
        "get_ipython().system('pip install numpy==1.23.5')"
      ],
      "id": "90b2b334",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.0\n",
            "Uninstalling numpy-1.26.0:\n",
            "  Successfully uninstalled numpy-1.26.0\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m No available output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Failed to build 'numpy' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eee9bd0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `numpy==1.23.5` failed due to build errors, indicating incompatibility with the current Python 3.12 environment. Since `d2l` requires `numpy>=1.23.5`, the next step is to install the latest compatible `numpy` version that successfully builds and then attempt to install `d2l` again. First, ensure any existing `numpy` installation is removed.\n",
        "\n"
      ],
      "id": "3eee9bd0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a259d0e",
        "outputId": "2d66c93a-ad16-4b10-e3a4-26811781d0c1"
      },
      "source": [
        "get_ipython().system('pip uninstall -y numpy')\n",
        "print(\"Uninstalled numpy to ensure a clean state.\")"
      ],
      "id": "3a259d0e",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mUninstalled numpy to ensure a clean state.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cef8526"
      },
      "source": [
        "**Reasoning**:\n",
        "Since previous attempts to install specific `numpy` versions (`1.23.5`, `1.26.0`) resulted in build errors or dependency conflicts, the next logical step is to install `numpy` without specifying a version. This allows `pip` to determine and install the latest version that is compatible with the current Python 3.12 environment and existing packages. After a successful `numpy` installation, the `d2l` package can then be installed.\n",
        "\n"
      ],
      "id": "2cef8526"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "7f455128",
        "outputId": "c1561294-3c0b-4d12-eb53-be9144557675"
      },
      "source": [
        "get_ipython().system('pip install numpy')\n",
        "get_ipython().system('pip install d2l')"
      ],
      "id": "7f455128",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "17c775779e364a3c8ba468a24197cdad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Using cached d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Collecting jupyter==1.0.0 (from d2l)\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting numpy==1.23.5 (from d2l)\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8010b2e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `numpy` without specifying a version resulted in `numpy==2.3.5`, which caused dependency conflicts. Also, `d2l` explicitly tried to install `numpy==1.23.5`, which failed to build. To resolve these issues, the next step is to ensure `numpy` is fully uninstalled, then explicitly install a `numpy` version known to be compatible with Python 3.12 and also satisfy `d2l`'s `numpy>=1.23.5` requirement. `numpy==1.26.6` is a good candidate. After successfully installing `numpy`, `d2l` can then be installed.\n",
        "\n"
      ],
      "id": "c8010b2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd374e65",
        "outputId": "7e65d08e-e6be-415d-b90c-3afd40461ef8"
      },
      "source": [
        "get_ipython().system('pip uninstall -y numpy')\n",
        "get_ipython().system('pip install numpy==1.26.6')\n",
        "get_ipython().system('pip install d2l')"
      ],
      "id": "bd374e65",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.3.5\n",
            "Uninstalling numpy-2.3.5:\n",
            "  Successfully uninstalled numpy-2.3.5\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.26.6 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.26.6\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting d2l\n",
            "  Using cached d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Collecting jupyter==1.0.0 (from d2l)\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting numpy==1.23.5 (from d2l)\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m No available output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Failed to build 'numpy' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd7af7b9"
      },
      "source": [
        "# Task\n",
        "Define the `Accumulator` class and `accuracy` function, which are commonly found in the `d2l` library for tracking and calculating training metrics."
      ],
      "id": "fd7af7b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe467d11"
      },
      "source": [
        "## Define Helper Functions\n",
        "\n",
        "### Subtask:\n",
        "Define the `Accumulator` class and `accuracy` function, which are essential for tracking and calculating training metrics during the process. These are commonly found in the `d2l` library for convenience.\n"
      ],
      "id": "fe467d11"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f88eee3"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `Accumulator` class and the `accuracy` function. This code block will implement both as specified in the instructions, including the `__init__`, `add`, `reset`, and `__getitem__` methods for `Accumulator`, and the logic for calculating accuracy from `y_hat` and `y`.\n",
        "\n"
      ],
      "id": "5f88eee3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b505c2a",
        "outputId": "7a854492-9636-493c-f178-545d6ffb24da"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def accuracy(y_hat, y):\n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = y_hat.argmax(axis=1)\n",
        "    cmp = y_hat.type(y.dtype) == y\n",
        "    return float(cmp.type(y.dtype).sum())\n",
        "\n",
        "print(\"Accumulator class and accuracy function defined.\")"
      ],
      "id": "6b505c2a",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulator class and accuracy function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a38f715b"
      },
      "source": [
        "## Define Evaluation Function\n",
        "\n",
        "### Subtask:\n",
        "Create the `evaluate_accuracy_gpu` function. This function will be responsible for calculating the model's accuracy on a given data iterator (like the test set) using the specified device (GPU or CPU).\n"
      ],
      "id": "a38f715b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ca0db1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `evaluate_accuracy_gpu` function, which calculates the model's accuracy on a given data iterator using a specified device. This code block will implement the function as per the instructions, utilizing the `Accumulator` class and `accuracy` function defined earlier.\n",
        "\n"
      ],
      "id": "a1ca0db1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29b6784",
        "outputId": "bce4d6d4-c415-4060-81be-2a97be9b7153"
      },
      "source": [
        "import torch\n",
        "\n",
        "def evaluate_accuracy_gpu(net, data_iter, device):\n",
        "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    metric = Accumulator(2)  # num_corrected_examples, num_examples\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            metric.add(accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "print(\"evaluate_accuracy_gpu function defined.\")"
      ],
      "id": "d29b6784",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate_accuracy_gpu function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0e194fd"
      },
      "source": [
        "## Modify Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Integrate the `Accumulator`, `accuracy`, and `evaluate_accuracy_gpu` functions into your `train_ch6` function. This will allow it to track the training loss and accuracy, calculate the test accuracy at the end of each epoch, and print these metrics.\n"
      ],
      "id": "e0e194fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c823a222"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Helper Functions Defined**: The `Accumulator` class, designed for accumulating sums over multiple variables, and the `accuracy` function, which computes the number of correct predictions, were successfully defined.\n",
        "*   **GPU-Enabled Evaluation Function**: The `evaluate_accuracy_gpu` function was correctly implemented. This function calculates model accuracy on a dataset using a specified device (GPU or CPU), sets the model to evaluation mode, disables gradient calculation, and utilizes the `Accumulator` and `accuracy` functions to track metrics.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The defined helper functions and the `evaluate_accuracy_gpu` function are now ready to be integrated into the `train_ch6` function to enhance its metric tracking and evaluation capabilities.\n"
      ],
      "id": "c823a222"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}